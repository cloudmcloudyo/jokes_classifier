{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Reddit NPL Classfication Challenge\n",
    "Notebook 1: Data Scraping\n",
    "\n",
    "This notebook serves as the starting point for the Reddit NPL Classification Challenge. It contains the webscraping of data from two subreddits on redditc.com using Pushshift API. The data will later serve as the training and testing set for this project. \n",
    "\n",
    "\n",
    "Content of this notebook include:  \n",
    "\n",
    "- [Data Scraping](#Data-Scraping)\n",
    "- [Initial Feature Selection](#Initial-Feature-Selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project would use requests library to pull the data from Pushshift API an use time library to control the pace so that my requests won't flood the application. As the datasets will be text-heavy, I'm also setting the display option to max column width so that I can review the content easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit to my classmate Devin\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pushshift API has a limit of 100 posts per request, I would need multiple requests to retrieve enough data for my models. My goal is to retrieve around 10,000 posts for each subreddit. Therefore I built a function to automate this process. This function will achieve the following: \n",
    "\n",
    "1. to retrieve the posts based on the subreddit name and total posts that I input; \n",
    "2. to concatenate all the posts retrieved per each subreddit into one dataframe; \n",
    "3. to drop the posts that have been removed and thus show no selftext in the system. \n",
    "\n",
    "I utilized the while loop to achieve these so that we significantly improve my efficiency in retrieving data. I printed out a status update after each request is done to indicate how many legit(non-removed) posts I have retrieved in total so that I could spot if anything went wrong. This turns out to be very helpful since I can spot if anything goes wrong in a much easier way. For example, when I was test running once, I noticed that the total posts I retrieve started to decrease as requests go on, which does not make sense. I then realized that it was because I did not reset the index after I dropped those \"removed\" posts so newly added posts were kept being dropped.\n",
    "\n",
    "I set the latest post to be 10/10/2020 midnight PDT, which is one week before the time the data is being retrieved. I did this because I believe posts with certain \"age\" may be a better representation of the subreddits since the new posts are usually subject to certain inspections by reddit and tend not have enough interactions(comments, upvotes, etc).\n",
    "\n",
    "At the end of the loop, I set the time interval of each request to 10s utilizing time.sleep function to show respect to the API community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(subreddit, total_size):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    params = {\n",
    "        'subreddit': subreddit,\n",
    "        'size': 100, # the maximum posts Pushlift allows per each request\n",
    "        'before': 1602313200  # 10/10/2020 00:00:00 PDT\n",
    "    }\n",
    "    df = pd.DataFrame()\n",
    "    total = 0 # implies how many \n",
    "    times = 0\n",
    "    while total < total_size:\n",
    "        res = requests.get(url, params)\n",
    "        if res.status_code == 200:\n",
    "            times += 1\n",
    "            posts = res.json()['data']\n",
    "            params['before'] = posts[99]['created_utc']\n",
    "            df_po = pd.DataFrame(posts)\n",
    "            df = pd.concat([df, df_po])\n",
    "            df.reset_index(drop = True, inplace = True)\n",
    "            df = df.drop(df[df['selftext'] == '[removed]'].index)\n",
    "            df.reset_index(drop = True, inplace = True)\n",
    "            total = df.shape[0]\n",
    "            print (f'This is no.{times} batch, status: {res.status_code}, total {total} data retrieved')\n",
    "\n",
    "        else:\n",
    "            print(res.status_code)\n",
    "        time.sleep(10)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is no.1 batch, status: 200, total 84 data retrieved\n",
      "This is no.2 batch, status: 200, total 170 data retrieved\n",
      "This is no.3 batch, status: 200, total 258 data retrieved\n",
      "This is no.4 batch, status: 200, total 344 data retrieved\n",
      "This is no.5 batch, status: 200, total 428 data retrieved\n",
      "This is no.6 batch, status: 200, total 508 data retrieved\n",
      "This is no.7 batch, status: 200, total 597 data retrieved\n",
      "This is no.8 batch, status: 200, total 685 data retrieved\n",
      "This is no.9 batch, status: 200, total 771 data retrieved\n",
      "This is no.10 batch, status: 200, total 859 data retrieved\n",
      "This is no.11 batch, status: 200, total 946 data retrieved\n",
      "This is no.12 batch, status: 200, total 1027 data retrieved\n",
      "This is no.13 batch, status: 200, total 1115 data retrieved\n",
      "This is no.14 batch, status: 200, total 1203 data retrieved\n",
      "This is no.15 batch, status: 200, total 1288 data retrieved\n",
      "This is no.16 batch, status: 200, total 1370 data retrieved\n",
      "This is no.17 batch, status: 200, total 1456 data retrieved\n",
      "This is no.18 batch, status: 200, total 1547 data retrieved\n",
      "This is no.19 batch, status: 200, total 1630 data retrieved\n",
      "This is no.20 batch, status: 200, total 1726 data retrieved\n",
      "This is no.21 batch, status: 200, total 1812 data retrieved\n",
      "This is no.22 batch, status: 200, total 1884 data retrieved\n",
      "This is no.23 batch, status: 200, total 1966 data retrieved\n",
      "This is no.24 batch, status: 200, total 2060 data retrieved\n",
      "This is no.25 batch, status: 200, total 2142 data retrieved\n",
      "This is no.26 batch, status: 200, total 2229 data retrieved\n",
      "This is no.27 batch, status: 200, total 2318 data retrieved\n",
      "This is no.28 batch, status: 200, total 2404 data retrieved\n",
      "This is no.29 batch, status: 200, total 2495 data retrieved\n",
      "This is no.30 batch, status: 200, total 2584 data retrieved\n",
      "This is no.31 batch, status: 200, total 2666 data retrieved\n",
      "This is no.32 batch, status: 200, total 2758 data retrieved\n",
      "This is no.33 batch, status: 200, total 2852 data retrieved\n",
      "This is no.34 batch, status: 200, total 2940 data retrieved\n",
      "This is no.35 batch, status: 200, total 3036 data retrieved\n",
      "This is no.36 batch, status: 200, total 3120 data retrieved\n",
      "This is no.37 batch, status: 200, total 3210 data retrieved\n",
      "This is no.38 batch, status: 200, total 3296 data retrieved\n",
      "This is no.39 batch, status: 200, total 3380 data retrieved\n",
      "This is no.40 batch, status: 200, total 3462 data retrieved\n",
      "This is no.41 batch, status: 200, total 3544 data retrieved\n",
      "This is no.42 batch, status: 200, total 3635 data retrieved\n",
      "This is no.43 batch, status: 200, total 3726 data retrieved\n",
      "This is no.44 batch, status: 200, total 3813 data retrieved\n",
      "This is no.45 batch, status: 200, total 3905 data retrieved\n",
      "This is no.46 batch, status: 200, total 3991 data retrieved\n",
      "This is no.47 batch, status: 200, total 4063 data retrieved\n",
      "This is no.48 batch, status: 200, total 4150 data retrieved\n",
      "This is no.49 batch, status: 200, total 4239 data retrieved\n",
      "This is no.50 batch, status: 200, total 4329 data retrieved\n",
      "This is no.51 batch, status: 200, total 4415 data retrieved\n",
      "This is no.52 batch, status: 200, total 4502 data retrieved\n",
      "This is no.53 batch, status: 200, total 4591 data retrieved\n",
      "This is no.54 batch, status: 200, total 4682 data retrieved\n",
      "This is no.55 batch, status: 200, total 4761 data retrieved\n",
      "This is no.56 batch, status: 200, total 4848 data retrieved\n",
      "This is no.57 batch, status: 200, total 4933 data retrieved\n",
      "This is no.58 batch, status: 200, total 5023 data retrieved\n",
      "This is no.59 batch, status: 200, total 5105 data retrieved\n",
      "This is no.60 batch, status: 200, total 5182 data retrieved\n",
      "This is no.61 batch, status: 200, total 5270 data retrieved\n",
      "This is no.62 batch, status: 200, total 5360 data retrieved\n",
      "This is no.63 batch, status: 200, total 5449 data retrieved\n",
      "This is no.64 batch, status: 200, total 5538 data retrieved\n",
      "This is no.65 batch, status: 200, total 5619 data retrieved\n",
      "This is no.66 batch, status: 200, total 5705 data retrieved\n",
      "This is no.67 batch, status: 200, total 5797 data retrieved\n",
      "This is no.68 batch, status: 200, total 5890 data retrieved\n",
      "This is no.69 batch, status: 200, total 5981 data retrieved\n",
      "This is no.70 batch, status: 200, total 6068 data retrieved\n",
      "This is no.71 batch, status: 200, total 6158 data retrieved\n",
      "This is no.72 batch, status: 200, total 6240 data retrieved\n",
      "This is no.73 batch, status: 200, total 6325 data retrieved\n",
      "This is no.74 batch, status: 200, total 6419 data retrieved\n",
      "This is no.75 batch, status: 200, total 6506 data retrieved\n",
      "This is no.76 batch, status: 200, total 6594 data retrieved\n",
      "This is no.77 batch, status: 200, total 6689 data retrieved\n",
      "This is no.78 batch, status: 200, total 6772 data retrieved\n",
      "This is no.79 batch, status: 200, total 6860 data retrieved\n",
      "This is no.80 batch, status: 200, total 6942 data retrieved\n",
      "This is no.81 batch, status: 200, total 7032 data retrieved\n",
      "This is no.82 batch, status: 200, total 7114 data retrieved\n",
      "This is no.83 batch, status: 200, total 7195 data retrieved\n",
      "This is no.84 batch, status: 200, total 7282 data retrieved\n",
      "This is no.85 batch, status: 200, total 7369 data retrieved\n",
      "This is no.86 batch, status: 200, total 7465 data retrieved\n",
      "This is no.87 batch, status: 200, total 7553 data retrieved\n",
      "This is no.88 batch, status: 200, total 7631 data retrieved\n",
      "This is no.89 batch, status: 200, total 7716 data retrieved\n",
      "This is no.90 batch, status: 200, total 7803 data retrieved\n",
      "This is no.91 batch, status: 200, total 7894 data retrieved\n",
      "This is no.92 batch, status: 200, total 7976 data retrieved\n",
      "This is no.93 batch, status: 200, total 8067 data retrieved\n",
      "This is no.94 batch, status: 200, total 8165 data retrieved\n",
      "This is no.95 batch, status: 200, total 8250 data retrieved\n",
      "This is no.96 batch, status: 200, total 8333 data retrieved\n",
      "This is no.97 batch, status: 200, total 8422 data retrieved\n",
      "This is no.98 batch, status: 200, total 8510 data retrieved\n",
      "This is no.99 batch, status: 200, total 8601 data retrieved\n",
      "This is no.100 batch, status: 200, total 8691 data retrieved\n",
      "This is no.101 batch, status: 200, total 8785 data retrieved\n",
      "This is no.102 batch, status: 200, total 8879 data retrieved\n",
      "This is no.103 batch, status: 200, total 8960 data retrieved\n",
      "This is no.104 batch, status: 200, total 9047 data retrieved\n",
      "This is no.105 batch, status: 200, total 9139 data retrieved\n",
      "This is no.106 batch, status: 200, total 9220 data retrieved\n",
      "This is no.107 batch, status: 200, total 9311 data retrieved\n",
      "This is no.108 batch, status: 200, total 9401 data retrieved\n",
      "This is no.109 batch, status: 200, total 9484 data retrieved\n",
      "This is no.110 batch, status: 200, total 9573 data retrieved\n",
      "This is no.111 batch, status: 200, total 9658 data retrieved\n",
      "This is no.112 batch, status: 200, total 9737 data retrieved\n",
      "This is no.113 batch, status: 200, total 9830 data retrieved\n",
      "This is no.114 batch, status: 200, total 9920 data retrieved\n",
      "This is no.115 batch, status: 200, total 10008 data retrieved\n"
     ]
    }
   ],
   "source": [
    "jokes = get_data('jokes', 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took me 115 requests to get 10,000+ posts that contains actual texts, which means the jokes subreddit posts tend to be kept there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is no.1 batch, status: 200, total 34 data retrieved\n",
      "This is no.2 batch, status: 200, total 68 data retrieved\n",
      "This is no.3 batch, status: 200, total 97 data retrieved\n",
      "This is no.4 batch, status: 200, total 125 data retrieved\n",
      "This is no.5 batch, status: 200, total 161 data retrieved\n",
      "This is no.6 batch, status: 200, total 190 data retrieved\n",
      "This is no.7 batch, status: 200, total 215 data retrieved\n",
      "This is no.8 batch, status: 200, total 246 data retrieved\n",
      "This is no.9 batch, status: 200, total 270 data retrieved\n",
      "This is no.10 batch, status: 200, total 306 data retrieved\n",
      "This is no.11 batch, status: 200, total 331 data retrieved\n",
      "This is no.12 batch, status: 200, total 356 data retrieved\n",
      "This is no.13 batch, status: 200, total 399 data retrieved\n",
      "This is no.14 batch, status: 200, total 435 data retrieved\n",
      "This is no.15 batch, status: 200, total 468 data retrieved\n",
      "This is no.16 batch, status: 200, total 515 data retrieved\n",
      "This is no.17 batch, status: 200, total 551 data retrieved\n",
      "This is no.18 batch, status: 200, total 590 data retrieved\n",
      "This is no.19 batch, status: 200, total 638 data retrieved\n",
      "This is no.20 batch, status: 200, total 681 data retrieved\n",
      "This is no.21 batch, status: 200, total 716 data retrieved\n",
      "This is no.22 batch, status: 200, total 760 data retrieved\n",
      "This is no.23 batch, status: 200, total 798 data retrieved\n",
      "This is no.24 batch, status: 200, total 832 data retrieved\n",
      "This is no.25 batch, status: 200, total 870 data retrieved\n",
      "This is no.26 batch, status: 200, total 909 data retrieved\n",
      "This is no.27 batch, status: 200, total 940 data retrieved\n",
      "This is no.28 batch, status: 200, total 980 data retrieved\n",
      "This is no.29 batch, status: 200, total 1025 data retrieved\n",
      "This is no.30 batch, status: 200, total 1059 data retrieved\n",
      "This is no.31 batch, status: 200, total 1093 data retrieved\n",
      "This is no.32 batch, status: 200, total 1133 data retrieved\n",
      "This is no.33 batch, status: 200, total 1164 data retrieved\n",
      "This is no.34 batch, status: 200, total 1200 data retrieved\n",
      "This is no.35 batch, status: 200, total 1226 data retrieved\n",
      "This is no.36 batch, status: 200, total 1248 data retrieved\n",
      "This is no.37 batch, status: 200, total 1276 data retrieved\n",
      "This is no.38 batch, status: 200, total 1329 data retrieved\n",
      "This is no.39 batch, status: 200, total 1379 data retrieved\n",
      "This is no.40 batch, status: 200, total 1424 data retrieved\n",
      "This is no.41 batch, status: 200, total 1471 data retrieved\n",
      "This is no.42 batch, status: 200, total 1515 data retrieved\n",
      "This is no.43 batch, status: 200, total 1555 data retrieved\n",
      "This is no.44 batch, status: 200, total 1614 data retrieved\n",
      "This is no.45 batch, status: 200, total 1656 data retrieved\n",
      "This is no.46 batch, status: 200, total 1713 data retrieved\n",
      "This is no.47 batch, status: 200, total 1759 data retrieved\n",
      "This is no.48 batch, status: 200, total 1807 data retrieved\n",
      "This is no.49 batch, status: 200, total 1860 data retrieved\n",
      "This is no.50 batch, status: 200, total 1919 data retrieved\n",
      "This is no.51 batch, status: 200, total 1981 data retrieved\n",
      "This is no.52 batch, status: 200, total 2035 data retrieved\n",
      "This is no.53 batch, status: 200, total 2095 data retrieved\n",
      "This is no.54 batch, status: 200, total 2147 data retrieved\n",
      "This is no.55 batch, status: 200, total 2189 data retrieved\n",
      "This is no.56 batch, status: 200, total 2234 data retrieved\n",
      "This is no.57 batch, status: 200, total 2297 data retrieved\n",
      "This is no.58 batch, status: 200, total 2358 data retrieved\n",
      "This is no.59 batch, status: 200, total 2414 data retrieved\n",
      "This is no.60 batch, status: 200, total 2465 data retrieved\n",
      "This is no.61 batch, status: 200, total 2527 data retrieved\n",
      "This is no.62 batch, status: 200, total 2578 data retrieved\n",
      "This is no.63 batch, status: 200, total 2637 data retrieved\n",
      "This is no.64 batch, status: 200, total 2691 data retrieved\n",
      "This is no.65 batch, status: 200, total 2740 data retrieved\n",
      "This is no.66 batch, status: 200, total 2807 data retrieved\n",
      "This is no.67 batch, status: 200, total 2877 data retrieved\n",
      "This is no.68 batch, status: 200, total 2932 data retrieved\n",
      "This is no.69 batch, status: 200, total 2998 data retrieved\n",
      "This is no.70 batch, status: 200, total 3050 data retrieved\n",
      "This is no.71 batch, status: 200, total 3114 data retrieved\n",
      "This is no.72 batch, status: 200, total 3176 data retrieved\n",
      "This is no.73 batch, status: 200, total 3246 data retrieved\n",
      "This is no.74 batch, status: 200, total 3301 data retrieved\n",
      "This is no.75 batch, status: 200, total 3360 data retrieved\n",
      "This is no.76 batch, status: 200, total 3424 data retrieved\n",
      "This is no.77 batch, status: 200, total 3479 data retrieved\n",
      "This is no.78 batch, status: 200, total 3533 data retrieved\n",
      "This is no.79 batch, status: 200, total 3591 data retrieved\n",
      "This is no.80 batch, status: 200, total 3647 data retrieved\n",
      "This is no.81 batch, status: 200, total 3701 data retrieved\n",
      "This is no.82 batch, status: 200, total 3754 data retrieved\n",
      "This is no.83 batch, status: 200, total 3825 data retrieved\n",
      "This is no.84 batch, status: 200, total 3869 data retrieved\n",
      "This is no.85 batch, status: 200, total 3917 data retrieved\n",
      "This is no.86 batch, status: 200, total 3980 data retrieved\n",
      "This is no.87 batch, status: 200, total 4035 data retrieved\n",
      "This is no.88 batch, status: 200, total 4094 data retrieved\n",
      "This is no.89 batch, status: 200, total 4145 data retrieved\n",
      "This is no.90 batch, status: 200, total 4205 data retrieved\n",
      "This is no.91 batch, status: 200, total 4263 data retrieved\n",
      "This is no.92 batch, status: 200, total 4319 data retrieved\n",
      "This is no.93 batch, status: 200, total 4376 data retrieved\n",
      "This is no.94 batch, status: 200, total 4420 data retrieved\n",
      "This is no.95 batch, status: 200, total 4468 data retrieved\n",
      "This is no.96 batch, status: 200, total 4524 data retrieved\n",
      "This is no.97 batch, status: 200, total 4577 data retrieved\n",
      "This is no.98 batch, status: 200, total 4637 data retrieved\n",
      "This is no.99 batch, status: 200, total 4701 data retrieved\n",
      "This is no.100 batch, status: 200, total 4756 data retrieved\n",
      "This is no.101 batch, status: 200, total 4820 data retrieved\n",
      "This is no.102 batch, status: 200, total 4871 data retrieved\n",
      "This is no.103 batch, status: 200, total 4939 data retrieved\n",
      "This is no.104 batch, status: 200, total 4990 data retrieved\n",
      "This is no.105 batch, status: 200, total 5038 data retrieved\n",
      "This is no.106 batch, status: 200, total 5100 data retrieved\n",
      "This is no.107 batch, status: 200, total 5164 data retrieved\n",
      "This is no.108 batch, status: 200, total 5224 data retrieved\n",
      "This is no.109 batch, status: 200, total 5281 data retrieved\n",
      "This is no.110 batch, status: 200, total 5327 data retrieved\n",
      "This is no.111 batch, status: 200, total 5390 data retrieved\n",
      "This is no.112 batch, status: 200, total 5441 data retrieved\n",
      "This is no.113 batch, status: 200, total 5489 data retrieved\n",
      "This is no.114 batch, status: 200, total 5535 data retrieved\n",
      "This is no.115 batch, status: 200, total 5586 data retrieved\n",
      "This is no.116 batch, status: 200, total 5640 data retrieved\n",
      "This is no.117 batch, status: 200, total 5678 data retrieved\n",
      "This is no.118 batch, status: 200, total 5732 data retrieved\n",
      "This is no.119 batch, status: 200, total 5782 data retrieved\n",
      "This is no.120 batch, status: 200, total 5830 data retrieved\n",
      "This is no.121 batch, status: 200, total 5885 data retrieved\n",
      "This is no.122 batch, status: 200, total 5918 data retrieved\n",
      "This is no.123 batch, status: 200, total 5963 data retrieved\n",
      "This is no.124 batch, status: 200, total 6011 data retrieved\n",
      "This is no.125 batch, status: 200, total 6052 data retrieved\n",
      "This is no.126 batch, status: 200, total 6092 data retrieved\n",
      "This is no.127 batch, status: 200, total 6140 data retrieved\n",
      "This is no.128 batch, status: 200, total 6188 data retrieved\n",
      "This is no.129 batch, status: 200, total 6225 data retrieved\n",
      "This is no.130 batch, status: 200, total 6275 data retrieved\n",
      "This is no.131 batch, status: 200, total 6312 data retrieved\n",
      "This is no.132 batch, status: 200, total 6360 data retrieved\n",
      "This is no.133 batch, status: 200, total 6411 data retrieved\n",
      "This is no.134 batch, status: 200, total 6457 data retrieved\n",
      "This is no.135 batch, status: 200, total 6502 data retrieved\n",
      "This is no.136 batch, status: 200, total 6557 data retrieved\n",
      "This is no.137 batch, status: 200, total 6602 data retrieved\n",
      "This is no.138 batch, status: 200, total 6638 data retrieved\n",
      "This is no.139 batch, status: 200, total 6679 data retrieved\n",
      "This is no.140 batch, status: 200, total 6729 data retrieved\n",
      "This is no.141 batch, status: 200, total 6777 data retrieved\n",
      "This is no.142 batch, status: 200, total 6822 data retrieved\n",
      "This is no.143 batch, status: 200, total 6874 data retrieved\n",
      "This is no.144 batch, status: 200, total 6914 data retrieved\n",
      "This is no.145 batch, status: 200, total 6967 data retrieved\n",
      "This is no.146 batch, status: 200, total 7006 data retrieved\n",
      "This is no.147 batch, status: 200, total 7050 data retrieved\n",
      "This is no.148 batch, status: 200, total 7091 data retrieved\n",
      "This is no.149 batch, status: 200, total 7136 data retrieved\n",
      "This is no.150 batch, status: 200, total 7183 data retrieved\n",
      "This is no.151 batch, status: 200, total 7235 data retrieved\n",
      "This is no.152 batch, status: 200, total 7293 data retrieved\n",
      "This is no.153 batch, status: 200, total 7345 data retrieved\n",
      "This is no.154 batch, status: 200, total 7395 data retrieved\n",
      "This is no.155 batch, status: 200, total 7433 data retrieved\n",
      "This is no.156 batch, status: 200, total 7483 data retrieved\n",
      "This is no.157 batch, status: 200, total 7525 data retrieved\n",
      "This is no.158 batch, status: 200, total 7578 data retrieved\n",
      "This is no.159 batch, status: 200, total 7624 data retrieved\n",
      "This is no.160 batch, status: 200, total 7687 data retrieved\n",
      "This is no.161 batch, status: 200, total 7741 data retrieved\n",
      "This is no.162 batch, status: 200, total 7788 data retrieved\n",
      "This is no.163 batch, status: 200, total 7834 data retrieved\n",
      "This is no.164 batch, status: 200, total 7885 data retrieved\n",
      "This is no.165 batch, status: 200, total 7946 data retrieved\n",
      "This is no.166 batch, status: 200, total 8000 data retrieved\n",
      "This is no.167 batch, status: 200, total 8040 data retrieved\n",
      "This is no.168 batch, status: 200, total 8092 data retrieved\n",
      "This is no.169 batch, status: 200, total 8127 data retrieved\n",
      "This is no.170 batch, status: 200, total 8182 data retrieved\n",
      "This is no.171 batch, status: 200, total 8229 data retrieved\n",
      "This is no.172 batch, status: 200, total 8283 data retrieved\n",
      "This is no.173 batch, status: 200, total 8335 data retrieved\n",
      "This is no.174 batch, status: 200, total 8400 data retrieved\n",
      "This is no.175 batch, status: 200, total 8454 data retrieved\n",
      "This is no.176 batch, status: 200, total 8499 data retrieved\n",
      "This is no.177 batch, status: 200, total 8552 data retrieved\n",
      "This is no.178 batch, status: 200, total 8603 data retrieved\n",
      "This is no.179 batch, status: 200, total 8653 data retrieved\n",
      "This is no.180 batch, status: 200, total 8707 data retrieved\n",
      "This is no.181 batch, status: 200, total 8772 data retrieved\n",
      "This is no.182 batch, status: 200, total 8831 data retrieved\n",
      "This is no.183 batch, status: 200, total 8891 data retrieved\n",
      "This is no.184 batch, status: 200, total 8952 data retrieved\n",
      "This is no.185 batch, status: 200, total 9006 data retrieved\n",
      "This is no.186 batch, status: 200, total 9058 data retrieved\n",
      "This is no.187 batch, status: 200, total 9107 data retrieved\n",
      "This is no.188 batch, status: 200, total 9162 data retrieved\n",
      "This is no.189 batch, status: 200, total 9225 data retrieved\n",
      "This is no.190 batch, status: 200, total 9290 data retrieved\n",
      "This is no.191 batch, status: 200, total 9347 data retrieved\n",
      "This is no.192 batch, status: 200, total 9410 data retrieved\n",
      "This is no.193 batch, status: 200, total 9477 data retrieved\n",
      "This is no.194 batch, status: 200, total 9527 data retrieved\n",
      "This is no.195 batch, status: 200, total 9582 data retrieved\n",
      "This is no.196 batch, status: 200, total 9644 data retrieved\n",
      "This is no.197 batch, status: 200, total 9705 data retrieved\n",
      "This is no.198 batch, status: 200, total 9753 data retrieved\n",
      "This is no.199 batch, status: 200, total 9806 data retrieved\n",
      "This is no.200 batch, status: 200, total 9858 data retrieved\n",
      "This is no.201 batch, status: 200, total 9918 data retrieved\n",
      "This is no.202 batch, status: 200, total 9974 data retrieved\n",
      "This is no.203 batch, status: 200, total 10030 data retrieved\n"
     ]
    }
   ],
   "source": [
    "tales = get_data('talesfromretail', 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the contrary, it took me 203 requests to retrieve enough posts for the tales from retail subreddit, which means there are lots of comes and goes in this subreddit. This might be because posts in tales from retail are usually real life stories and original posters may change their mind about sharing personal experience after a while, or maybe there posts that should belong to other subreddits made their appearance in the wrong group. As there are many tales from ... subreddits group, it could make sense too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jokes shape (10008, 73)\n",
      "Tales shape (10030, 88)\n"
     ]
    }
   ],
   "source": [
    "print(f'Jokes shape {jokes.shape}')\n",
    "print(f'Tales shape {tales.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after scraping the data, I have 10,000+ legit posts for both groups. I consider this to be enough for my later stages. However while comparing the shape of two subreddits, I noticed that tales from retail has a lot more columns in its dataset, which means me want to look into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As my problem statement is about classifying posts, many features that I retrieved from Pushlift API may not be relevant. Therefore I trimmed down some features and only keeping those that will be helpful for my EDA & modelings. Also per mentioned just above, there are columns differences between jokes and tales subreddits. So I started with examing the exact columns of both groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['all_awardings', 'allow_live_comments', 'author',\n",
       "       'author_flair_css_class', 'author_flair_richtext', 'author_flair_text',\n",
       "       'author_flair_type', 'author_fullname', 'author_patreon_flair',\n",
       "       'author_premium', 'awarders', 'can_mod_post', 'contest_mode',\n",
       "       'created_utc', 'domain', 'full_link', 'gildings', 'id',\n",
       "       'is_crosspostable', 'is_meta', 'is_original_content',\n",
       "       'is_reddit_media_domain', 'is_robot_indexable', 'is_self', 'is_video',\n",
       "       'link_flair_background_color', 'link_flair_richtext',\n",
       "       'link_flair_text_color', 'link_flair_type', 'locked', 'media_only',\n",
       "       'no_follow', 'num_comments', 'num_crossposts', 'over_18',\n",
       "       'parent_whitelist_status', 'permalink', 'pinned', 'pwls',\n",
       "       'removed_by_category', 'retrieved_on', 'score', 'selftext',\n",
       "       'send_replies', 'spoiler', 'stickied', 'subreddit', 'subreddit_id',\n",
       "       'subreddit_subscribers', 'subreddit_type', 'thumbnail', 'title',\n",
       "       'total_awards_received', 'treatment_tags', 'upvote_ratio', 'url',\n",
       "       'whitelist_status', 'wls', 'link_flair_css_class', 'link_flair_text',\n",
       "       'post_hint', 'preview', 'author_flair_background_color',\n",
       "       'author_flair_text_color', 'author_cakeday', 'banned_by',\n",
       "       'link_flair_template_id', 'crosspost_parent', 'crosspost_parent_list',\n",
       "       'url_overridden_by_dest', 'thumbnail_height', 'thumbnail_width',\n",
       "       'edited'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['all_awardings', 'allow_live_comments', 'author',\n",
       "       'author_flair_css_class', 'author_flair_richtext', 'author_flair_text',\n",
       "       'author_flair_type', 'author_fullname', 'author_patreon_flair',\n",
       "       'author_premium', 'awarders', 'can_mod_post', 'contest_mode',\n",
       "       'created_utc', 'domain', 'full_link', 'gildings', 'id',\n",
       "       'is_crosspostable', 'is_meta', 'is_original_content',\n",
       "       'is_reddit_media_domain', 'is_robot_indexable', 'is_self', 'is_video',\n",
       "       'link_flair_background_color', 'link_flair_css_class',\n",
       "       'link_flair_richtext', 'link_flair_text_color', 'link_flair_type',\n",
       "       'locked', 'media_only', 'no_follow', 'num_comments', 'num_crossposts',\n",
       "       'over_18', 'parent_whitelist_status', 'permalink', 'pinned', 'pwls',\n",
       "       'retrieved_on', 'score', 'selftext', 'send_replies', 'spoiler',\n",
       "       'stickied', 'subreddit', 'subreddit_id', 'subreddit_subscribers',\n",
       "       'subreddit_type', 'thumbnail', 'title', 'total_awards_received',\n",
       "       'treatment_tags', 'upvote_ratio', 'url', 'whitelist_status', 'wls',\n",
       "       'removed_by_category', 'link_flair_text',\n",
       "       'author_flair_background_color', 'author_flair_text_color', 'banned_by',\n",
       "       'author_flair_template_id', 'post_hint', 'preview', 'distinguished',\n",
       "       'suggested_sort', 'edited', 'link_flair_template_id', 'author_cakeday',\n",
       "       'crosspost_parent', 'crosspost_parent_list', 'url_overridden_by_dest',\n",
       "       'gilded', 'steward_reports', 'removed_by', 'updated_utc',\n",
       "       'og_description', 'og_title', 'thumbnail_height', 'thumbnail_width',\n",
       "       'author_created_utc', 'category', 'content_categories', 'media_embed',\n",
       "       'removal_reason', 'secure_media_embed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tales.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used set to find the discrepency columns that tales has but jokes doesn't -- they don't seem to be very much irrelvant to my analysis. Therefore I decided not to match them in the jokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author_created_utc',\n",
       " 'author_flair_template_id',\n",
       " 'category',\n",
       " 'content_categories',\n",
       " 'distinguished',\n",
       " 'gilded',\n",
       " 'media_embed',\n",
       " 'og_description',\n",
       " 'og_title',\n",
       " 'removal_reason',\n",
       " 'removed_by',\n",
       " 'secure_media_embed',\n",
       " 'steward_reports',\n",
       " 'suggested_sort',\n",
       " 'updated_utc'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tales.columns) - set(jokes.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I handpicked some features that I believe important to my EDA and modeling. The most important among them of course will be the columns with texts, namely title and selftext. I also included score, number of comments, upvote ratio and created utc so that I can have an idea what kind of post it is and better understand the context that the analysis is based on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['author', 'title', 'selftext', 'score', 'num_comments', 'upvote_ratio', 'created_utc', 'subreddit' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_pick = jokes[features]\n",
    "tales_pick = tales[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing I want to check here is to see if there are any duplicates in the text columns. And if there are, I would drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_pick = jokes_pick.drop_duplicates('title', keep = 'first')\n",
    "jokes_pick = jokes_pick.drop_duplicates('selftext', keep = 'first')\n",
    "jokes_pick.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tales_pick = tales_pick.drop_duplicates('title', keep = 'first')\n",
    "tales_pick = tales_pick.drop_duplicates('selftext', keep = 'first')\n",
    "tales_pick.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jokes new shape (9526, 8)\n",
      "Tales new shape (9434, 8)\n"
     ]
    }
   ],
   "source": [
    "print(f'Jokes new shape {jokes_pick.shape}')\n",
    "print(f'Tales new shape {tales_pick.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There I have it. Seems there are not significant duplicate posts in either subreddits. With around 9500 posts for each subreddit, I'm comfortable to move forward to the next stage of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_pick.to_csv('../data/jokes.csv', index = False)\n",
    "tales_pick.to_csv('../data/talesfromretail.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
